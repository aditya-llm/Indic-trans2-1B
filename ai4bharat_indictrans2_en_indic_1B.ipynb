{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMCOcflT2xTx4S8Bq44imw5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya-llm/Indic-trans2-1B/blob/main/ai4bharat_indictrans2_en_indic_1B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Aa-nRCzPVdF"
      },
      "source": [
        "# IndicTrans2 HF Inference\n",
        "\n",
        "We provide an example notebook on how to use our IndicTrans2 models which were originally trained with the fairseq to HuggingFace transformers for inference purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfsv02IeP2It"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Please run the cells below to install the necessary dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWw7E8VwUPFz"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/AI4Bharat/IndicTrans2.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%cd /content/IndicTrans2/huggingface_interface"
      ],
      "metadata": {
        "id": "yk07KwZ-UhDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python3 -m pip install nltk sacremoses pandas regex mock transformers==4.53.2 mosestokenizer\n",
        "!python3 -c \"import nltk; nltk.download('punkt')\"\n",
        "!python3 -m pip install bitsandbytes scipy accelerate datasets\n",
        "!python3 -m pip install sentencepiece\n",
        "\n",
        "!git clone https://github.com/VarunGumma/IndicTransToolkit.git\n",
        "%cd IndicTransToolkit\n",
        "!python3 -m pip install --editable ./\n",
        "%cd .."
      ],
      "metadata": {
        "id": "UEcg_f6HUkC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjN7ub1tO33H"
      },
      "source": [
        "**IMPORTANT : Restart your run-time first and then run the cells below.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig, AutoTokenizer\n",
        "from IndicTransToolkit.processor import IndicProcessor\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "quantization = None"
      ],
      "metadata": {
        "id": "h6ayyvd5UtCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model_and_tokenizer(ckpt_dir, quantization):\n",
        "    if quantization == \"4-bit\":\n",
        "        qconfig = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "    elif quantization == \"8-bit\":\n",
        "        qconfig = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,\n",
        "            bnb_8bit_use_double_quant=True,\n",
        "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "    else:\n",
        "        qconfig = None\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        ckpt_dir,\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True,\n",
        "        quantization_config=qconfig,\n",
        "    )\n",
        "\n",
        "    if qconfig == None:\n",
        "        model = model.to(DEVICE)\n",
        "        if DEVICE == \"cuda\":\n",
        "            model.half()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n",
        "    translations = []\n",
        "    for i in range(0, len(input_sentences), BATCH_SIZE):\n",
        "        batch = input_sentences[i : i + BATCH_SIZE]\n",
        "\n",
        "        # Preprocess the batch and extract entity mappings\n",
        "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "\n",
        "        # Tokenize the batch and generate input encodings\n",
        "        inputs = tokenizer(\n",
        "            batch,\n",
        "            truncation=True,\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\",\n",
        "            return_attention_mask=True,\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # Generate translations using the model\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                use_cache=True,\n",
        "                min_length=0,\n",
        "                max_length=256,\n",
        "                num_beams=5,\n",
        "                num_return_sequences=1,\n",
        "            )\n",
        "\n",
        "        # Decode the generated tokens into text\n",
        "        generated_tokens = tokenizer.batch_decode(\n",
        "            generated_tokens,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=True,\n",
        "        )\n",
        "\n",
        "        # Postprocess the translations, including entity replacement\n",
        "        translations += ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
        "\n",
        "        del inputs\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return translations"
      ],
      "metadata": {
        "id": "_efpgTvYU-Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erNCuZTEMt49"
      },
      "source": [
        "### English to Indic Example\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"  # ai4bharat/indictrans2-en-indic-dist-200M\n",
        "en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, quantization)\n",
        "\n",
        "ip = IndicProcessor(inference=True)\n",
        "\n",
        "en_sents = [\n",
        "    \"When I was young, I used to go to the park every day.\",\n",
        "    \"He has many old books, which he inherited from his ancestors.\",\n",
        "    \"I can't figure out how to solve my problem.\",\n",
        "    \"She is very hardworking and intelligent, which is why she got all the good marks.\",\n",
        "    \"We watched a new movie last week, which was very inspiring.\",\n",
        "    \"If you had met me at that time, we would have gone out to eat.\",\n",
        "    \"She went to the market with her sister to buy a new sari.\",\n",
        "    \"Raj told me that he is going to his grandmother's house next month.\",\n",
        "    \"All the kids were having fun at the party and were eating lots of sweets.\",\n",
        "    \"My friend has invited me to his birthday party, and I will give him a gift.\",\n",
        "]\n",
        "\n",
        "src_lang, tgt_lang = \"eng_Latn\", \"hin_Deva\"\n",
        "hi_translations = batch_translate(en_sents, src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip)\n",
        "\n",
        "print(f\"\\n{src_lang} - {tgt_lang}\")\n",
        "for input_sentence, translation in zip(en_sents, hi_translations):\n",
        "    print(f\"{src_lang}: {input_sentence}\")\n",
        "    print(f\"{tgt_lang}: {translation}\")\n",
        "\n",
        "# flush the models to free the GPU memory\n",
        "# del en_indic_tokenizer, en_indic_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCLPheTaU_CV",
        "outputId": "a33102f2-cc44-4b11-f772-976a0b8b3489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "eng_Latn - hin_Deva\n",
            "eng_Latn: When I was young, I used to go to the park every day.\n",
            "hin_Deva: जब मैं छोटा था, मैं हर दिन पार्क जाता था।\n",
            "eng_Latn: He has many old books, which he inherited from his ancestors.\n",
            "hin_Deva: उनके पास कई पुरानी किताबें हैं, जो उन्हें अपने पूर्वजों से विरासत में मिली हैं।\n",
            "eng_Latn: I can't figure out how to solve my problem.\n",
            "hin_Deva: मुझे समझ नहीं आ रहा है कि मैं अपनी समस्या का समाधान कैसे करूं।\n",
            "eng_Latn: She is very hardworking and intelligent, which is why she got all the good marks.\n",
            "hin_Deva: वह बहुत मेहनती और बुद्धिमान है, यही कारण है कि उसे सभी अच्छे अंक मिले।\n",
            "eng_Latn: We watched a new movie last week, which was very inspiring.\n",
            "hin_Deva: हमने पिछले हफ्ते एक नई फिल्म देखी, जो बहुत प्रेरणादायक थी।\n",
            "eng_Latn: If you had met me at that time, we would have gone out to eat.\n",
            "hin_Deva: अगर आप उस समय मुझसे मिलते तो हम बाहर खाना खाने जाते।\n",
            "eng_Latn: She went to the market with her sister to buy a new sari.\n",
            "hin_Deva: वह अपनी बहन के साथ नई साड़ी खरीदने के लिए बाजार गई थी।\n",
            "eng_Latn: Raj told me that he is going to his grandmother's house next month.\n",
            "hin_Deva: राज ने मुझे बताया कि वह अगले महीने अपनी दादी के घर जा रहा है।\n",
            "eng_Latn: All the kids were having fun at the party and were eating lots of sweets.\n",
            "hin_Deva: पार्टी में सभी बच्चे खूब मस्ती कर रहे थे और खूब मिठाइयां खा रहे थे।\n",
            "eng_Latn: My friend has invited me to his birthday party, and I will give him a gift.\n",
            "hin_Deva: मेरे दोस्त ने मुझे अपने जन्मदिन की पार्टी में आमंत्रित किया है, और मैं उसे एक उपहार दूंगा।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your input/output folders\n",
        "input_folder = \"/content/drive/MyDrive/Colab Notebooks/task/cleaned_texts\"   # English text files\n",
        "output_folder = \"/content/drive/MyDrive/Colab Notebooks/task/Translated_texts\"  # Hindi output\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGo1BoBGWpMy",
        "outputId": "f539ec59-7225-46fc-9ffc-c455a407a2a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_lines(file_path):\n",
        "    \"\"\"Reads file, removes empty/noisy lines, and returns cleaned list of sentences.\"\"\"\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Remove empty lines and strip extra spaces\n",
        "    cleaned_lines = [line.strip() for line in lines if line.strip() != \"\"]\n",
        "    return cleaned_lines\n"
      ],
      "metadata": {
        "id": "qyEmEfENXRPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_file(input_file, output_file, src_lang, tgt_lang, model, tokenizer, ip):\n",
        "    # Clean lines\n",
        "    sentences = clean_text_lines(input_file)\n",
        "\n",
        "    if not sentences:\n",
        "        print(f\"⚠️ Skipping empty file: {input_file}\")\n",
        "        return\n",
        "\n",
        "    # Translate\n",
        "    translations = batch_translate(sentences, src_lang, tgt_lang, model, tokenizer, ip)\n",
        "\n",
        "    # Save output to new file\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in translations:\n",
        "            f.write(line + \"\\n\")\n",
        "\n",
        "    print(f\"✅ Translated file saved: {output_file}\")\n"
      ],
      "metadata": {
        "id": "ZrR3yzT2XVhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_lang, tgt_lang = \"eng_Latn\", \"hin_Deva\"\n",
        "\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        input_file = os.path.join(input_folder, filename)\n",
        "        output_file = os.path.join(output_folder, filename.replace(\"_cleaned\", \"_hindi\"))\n",
        "\n",
        "        translate_file(input_file, output_file, src_lang, tgt_lang,\n",
        "                       en_indic_model, en_indic_tokenizer, ip)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErLvllgMZKoT",
        "outputId": "bc94bd1a-2ffc-49b8-a2dd-f4cdd7ea02a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Translated file saved: /content/drive/MyDrive/Colab Notebooks/task/Translated_texts/hepr1ps_hindi.txt\n",
            "✅ Translated file saved: /content/drive/MyDrive/Colab Notebooks/task/Translated_texts/hepr103_hindi.txt\n"
          ]
        }
      ]
    }
  ]
}